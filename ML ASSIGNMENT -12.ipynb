{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447dc2df",
   "metadata": {},
   "source": [
    "1. What is prior probability? Give an example.\n",
    "2. What is posterior probability? Give an example.\n",
    "3. What is likelihood probability? Give an example.\n",
    "\n",
    "4. What is Naïve Bayes classifier? Why is it named so?\n",
    "\n",
    "5. What is optimal Bayes classifier?\n",
    "\n",
    "6. Write any two features of Bayesian learning methods.\n",
    "\n",
    "7. Define the concept of consistent learners.\n",
    "\n",
    "8. Write any two strengths of Bayes classifier.\n",
    "\n",
    "9. Write any two weaknesses of Bayes classifier.\n",
    "\n",
    "10. Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "1. Text classification\n",
    "\n",
    "2. Spam filtering\n",
    "\n",
    "3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd2619",
   "metadata": {},
   "source": [
    "1.A prior probability distribution of an uncertain quantity, often simply called the prior, is its assumed probability distribution before some evidence is taken into account. For example, the prior could be the probability distribution representing the relative proportions of voters who will vote for a particular politician in a future election."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c0099",
   "metadata": {},
   "source": [
    "2.Posterior probability is a revised probability that takes into account new available information. For example, let there be two urns, urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls. Now if an urn is selected at random, the probability that urn A is chosen is 0.5. This is the a priori probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da26b1",
   "metadata": {},
   "source": [
    "3.The probability of a chance or a probability of something. Example: There is a strong likelihood of him being the class monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee098bc",
   "metadata": {},
   "source": [
    "4.A Naïve Bayes classifier is a type of machine learning algorithm that uses Bayes’ theorem to predict the class of an object based on its features. It is called Naïve because it assumes that the features are independent of each other, which is often not true in real-world situations. For example, a Naïve Bayes classifier might predict whether an email is spam or not based on the presence of certain words, without considering the context or order of those words.\n",
    "\n",
    "Naïve Bayes classifiers are simple, fast, and effective for many classification tasks, especially those involving high-dimensional data such as text or images. They can also handle missing or noisy data by using smoothing techniques or prior probabilities. Some applications of Naïve Bayes classifiers are spam filtering, sentiment analysis, document categorization, and face recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32dcfb",
   "metadata": {},
   "source": [
    "5.The optimal Bayes classifier is a theoretical model that makes the most probable prediction for a new example, based on the training data and the space of hypotheses. It uses the Bayes theorem to calculate the posterior probability of each hypothesis given the data, and chooses the one with the highest probability. The optimal Bayes classifier is also called the Bayes optimal learner, the Bayes classifier, or the Bayes optimal decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ed4c7",
   "metadata": {},
   "source": [
    "6.Bayesian learning methods are a type of machine learning techniques that use Bayes’ theorem to update the probability of a hypothesis based on prior knowledge and observed data. Some features of Bayesian learning methods are:\n",
    "\n",
    "They can combine prior knowledge and observed data to determine the final probability of a hypothesis. This allows them to incorporate background information and domain knowledge into the learning process.\n",
    "They can accommodate hypotheses that make probabilistic predictions. This means that they can handle uncertainty and noise in the data, and provide a measure of confidence for their predictions.\n",
    "They can classify new instances by combining the predictions of multiple hypotheses, weighted by their probabilities. This is known as Bayesian model averaging, and it can improve the accuracy and robustness of the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c01f0",
   "metadata": {},
   "source": [
    "7.A consistent learner is a learning algorithm that always outputs a hypothesis that is error-free on the given training data, as long as such a hypothesis exists in the hypothesis space. In other words, a consistent learner finds a hypothesis that matches all the observed examples. A consistent learner is also called an explanatory learner, because it tries to explain the data with a simple hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e3e60",
   "metadata": {},
   "source": [
    "8.t is simple and easy to implement.\n",
    "It is fast and can be used to make real-time predictions.\n",
    "It handles both continuous and discrete data.\n",
    "It is highly scalable with the number of predictors and data points.\n",
    "It is not sensitive to irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76607c",
   "metadata": {},
   "source": [
    "9.In most situations, the feature show some form of dependency.\n",
    "  Zero probability problem : When we encounter words in the test data for a particular class that are not present in the        training  data, we might end up with zero class probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd573961",
   "metadata": {},
   "source": [
    "10.1.Collect a training set of labeled documents, where each document belongs to one of the predefined classes.\n",
    "Preprocess the documents by tokenizing, removing stopwords, stemming, etc.\n",
    "Calculate the prior probabilities of each class, i.e., the fraction of documents in the training set that belong to each class.\n",
    "Calculate the conditional probabilities of each word given each class, i.e., the fraction of documents in each class that contain each word.\n",
    "For a new document, preprocess it and calculate the posterior probabilities of each class, i.e., the probability of the document belonging to each class given the words in the document. This is done by applying Bayes’ theorem and multiplying the prior and conditional probabilities of each class and word.\n",
    "Assign the document to the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf3643",
   "metadata": {},
   "source": [
    "10.2.The algorithm calculates the probability of an email being spam or non-spam given the evidence (the words or tokens in the email) and then compares the probabilities to make a prediction. For example, if an email contains words like “free”, “guaranteed”, “click here”, etc., the algorithm will assign a higher probability of spam to the email than if it contains words like “meeting”, “project”, “deadline”, etc. The algorithm can be trained on a large dataset of labeled emails (spam or non-spam) to learn the probabilities of different words or tokens occurring in spam or non-spam emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05661cd",
   "metadata": {},
   "source": [
    "10.3.To perform market sentiment analysis using Naïve Bayes classifier, one needs to follow these steps:\n",
    "\n",
    "Collect and preprocess the data: The data can be obtained from various sources, such as social media, online reviews, news articles, etc. The data needs to be cleaned, tokenized, normalized, and vectorized. This means that the text data is converted into numerical vectors that represent the frequency or presence of certain words or phrases.\n",
    "Label the data: The data needs to be annotated with the class labels, such as positive, negative, or neutral. This can be done manually by human experts, or automatically by using some rules or heuristics. Alternatively, one can use existing labeled datasets that are available online.\n",
    "Train and test the model: The data is split into training and testing sets. The training set is used to estimate the parameters of the Naïve Bayes classifier, such as the prior probabilities of the classes and the conditional probabilities of the features given the classes. The testing set is used to evaluate the performance of the model, such as the accuracy, precision, recall, or F1-score.\n",
    "Interpret and improve the results: The results of the model can be analyzed and interpreted to understand the strengths and weaknesses of the model, and to identify the sources of errors or biases. The model can be improved by using different feature selection methods, smoothing techniques, or hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
